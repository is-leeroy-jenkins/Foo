###### foo
![](https://github.com/is-leeroy-jenkins/Foo/blob/main/resources/images/foo_project.png)


[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/is-leeroy-jenkins/Foo/blob/main/ipynb/soup.ipynb)

*A Modular Python Framework for Retrieval-Augmented Pipelines and Agentic Workflows*


[![Streamlit App](https://img.shields.io/badge/Streamlit-App-FF4B4B?logo=streamlit&logoColor=white)](https://boo-py.streamlit.app/)

![](https://github.com/is-leeroy-jenkins/Boo/blob/main/resources/images/Foo-streamlit.gif)

## ğŸ“š Table of Contents

* [Features](#features)
* [Architecture](#architecture)
* [Directory Structure](#directory-structure)
* [Installation](#installation)
* [Quick Start](#quick-start)
* [Usage Examples](#usage-examples)
* [Loaders](#loaders)
* [Fetchers](#fetchers)
* [Scrapers](#scrapers)
* [Dependencies](#dependencies)
* [Technical Notes](#technical-notes)
* [License](#license)



## âœ¨ Features

* Modular, pluggable pipeline for document, web, and data retrieval and processing.
* Robust, extensible loaders and fetchers for all common document and web data formats.
* Powerful HTML/text scraping and cleaning.
* Integrates with OpenAI, LangChain, ChromaDB, and advanced document stores.
* Strong type safety and error handling.
* Simple, extensible codebase.



## ğŸ›ï¸ Architecture

```
ğŸ“„ Fetcher â†’ ğŸ•¸ï¸ Scraper â†’ ğŸ“¤ Loader  
```



## ğŸ—‚ï¸ Directory Structure

```
foo/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ core.py
â”œâ”€â”€ data.py
â”œâ”€â”€ fetchers.py
â”œâ”€â”€ loaders.py
â”œâ”€â”€ scrapers.py
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
```



## ğŸ›¡ï¸ Installation

```bash

git clone https://github.com/is-leeroy-jenkins/Foo.git
cd Foo
python -m venv .venv
.venv/bin/pip install -r requirements.txt
```


---

## â–¶ï¸ Running the Streamlit App

```bash

streamlit run app.py
```

The application runs locally and does not require a database or background services.

---

## ğŸš€ Quick Start

```python
from foo.core import Fetch
fetcher = Fetch(model='gpt-4o', db_uri='sqlite:///foo.sqlite', doc_paths=['docs/*.pdf'])
response = fetcher.query_docs("Summarize the uploaded PDFs.")
print(response)
```



## ğŸ” Example Usage 

**Scrape Web Page Paragraphs:**

```python
from foo.scrapers import WebExtractor
extractor = WebExtractor()
paragraphs = extractor.scrape_paragraphs("https://example.com")
print(paragraphs)
```

**Load and Chunk a PDF:**

```python
from foo.loaders import PdfLoader
loader = PdfLoader()
doc = loader.load('docs/report.pdf')
chunks = loader.split(doc)
print(chunks)
```





## ğŸ—‚ï¸ Loader

*Abstract base class for all loaders; provides document loading and splitting interface.*

* `load(path)` â€“ Loads the document from the specified path.

* `split(doc, chunk=1000, overlap=100)` â€“ Splits a loaded document into overlapping text chunks.



## ğŸ§¾ CsvLoader

*Loads and splits CSV files for tabular data ingestion.*

* `load(path)` â€“ Loads and parses a CSV file.

* `split(doc, chunk=1000, overlap=100)` â€“ Splits CSV content for batch processing.



## ğŸ“„ PdfLoader

*Loads PDF files, supporting robust text extraction and chunking.*

* `load(path)` â€“ Loads and extracts text from a PDF document.

* `split(doc, chunk=1000, overlap=100)` â€“ Splits PDF text into manageable chunks.



## ğŸ“ DocxLoader

*Loads and extracts content from DOCX (Word) documents.*

* `load(path)` â€“ Loads and parses a DOCX file.

* `split(doc, chunk=1000, overlap=100)` â€“ Splits DOCX text for analysis.



## ğŸŒ HtmlLoader

*Loads and parses local HTML documents.*

* `load(path)` â€“ Loads HTML content from a file.

* `split(doc, chunk=1000, overlap=100)` â€“ Splits HTML body text into chunks.



## ğŸ“Š PptxLoader

*Loads and extracts text from PowerPoint (`.pptx`) files.*

* `load(path)` â€“ Loads slide contents from a PowerPoint file.

* `split(doc, chunk=1000, overlap=100)` â€“ Splits slide text for downstream use.



## ğŸ“ˆ ExcelLoader

*Loads and processes Excel spreadsheets (XLS/XLSX).*

* `load(path)` â€“ Loads and reads an Excel file.

* `split(doc, chunk=1000, overlap=100)` â€“ Splits spreadsheet content for batch processing.



## ğŸ“œ TextLoader

*Loads plain text files, supporting chunked analysis.*

* `load(path)` â€“ Loads the content of a text file.

* `split(doc, chunk=1000, overlap=100)` â€“ Splits text file content into chunks.



## ğŸ—ƒï¸ JsonLoader

*Loads structured data from JSON files.*

* `load(path)` â€“ Loads and parses JSON data.

* `split(doc, chunk=1000, overlap=100)` â€“ Splits JSON-encoded text as appropriate.



## ğŸ“ MarkdownLoader

*Loads and splits Markdown (`.md`) documents.*

* `load(path)` â€“ Loads a Markdown fileâ€™s content.

* `split(doc, chunk=1000, overlap=100)` â€“ Splits Markdown into logical text chunks.



## ğŸ—‚ï¸ XmlLoader

*Loads and parses XML documents.*

* `load(path)` â€“ Loads and parses XML content.

* `split(doc, chunk=1000, overlap=100)` â€“ Splits XML text nodes for further use.



## ğŸ–¼ï¸ ImageLoader

*Loads and processes image files for downstream tasks (e.g., OCR, embeddings).*

* `load(path)` â€“ Loads an image file.

* `split(doc, chunk=1000, overlap=100)` â€“ Optionally splits or processes image regions.



## ğŸ“º YouTubeLoader

*Loads YouTube video transcripts and metadata.*

* `load(path)` â€“ Retrieves transcript/caption text for a given video ID or URL.

* `split(doc, chunk=1000, overlap=100)` â€“ Splits transcript into chunks.



## ğŸ’¾ UnstructuredLoader

*Flexible loader for mixed-format or â€œmessyâ€ documents.*

* `load(path)` â€“ Loads and attempts to parse various unstructured document formats

* `split(doc, chunk=1000, overlap=100)` â€“ Splits extracted text for processing.





## ğŸ¤– Fetcher

*Abstract base class for all fetchers, defining the core fetch interface.*

* `fetch(url, **kwargs)` â€“ Performs a data retrieval request to a specified endpoint.



## ğŸŒ WebFetcher

*Fetches HTML content using `requests` and provides rich methods for extracting text and elements from web pages.*

* `fetch(url, time=10)` â€“ Performs an HTTP GET and returns a structured Result.

* `html_to_text(html)` â€“ Converts raw HTML to compact plain text.


## ğŸ•¸ï¸ WebCrawler

*JavaScript-capable crawler using `crawl4ai` or Playwright, for dynamic content.*

* `fetch(url, depth=1, **kwargs)` â€“ Recursively crawls and fetches HTML from linked pages.



## ğŸŒŒ StarMap

*Fetches celestial map images using coordinates from StarMap.org.*

* `fetch_by_coordinates(ra, dec)` â€“ Generates a star map based on right ascension and declination.



## ğŸ“š ArxivFetcher

*Loads arXiv papers via the `ArxivRetriever`, returning results as document objects.*

* `fetch(query, **kwargs)` â€“ Retrieves papers matching the specified query.



## ğŸ—‚ï¸ GoogleDriveFetcher

*Loads files from Google Drive using LangChain retrievers.*

* `fetch(query, **kwargs)` â€“ Retrieves documents or file metadata from Google Drive.



## ğŸ“– WikipediaFetcher

*Retrieves Wikipedia articles with full metadata support.*

* `fetch(query, **kwargs)` â€“ Retrieves article text and metadata for a search term.



## ğŸ“° NewsFetcher

*Fetches news articles using Thenewsapi.com.*

* `fetch(query, **kwargs)` â€“ Retrieves news articles based on keyword and category.



## ğŸ” GoogleSearch

*Uses Google Custom Search API for web search.*

* `fetch(query, **kwargs)` â€“ Executes a web search and returns the top results.



## ğŸ—ºï¸ GoogleMaps

*Integrates with Google Maps for geocoding, address validation, and directions.*

* `geocode(address)` â€“ Returns geocoordinates for a given address.

* `directions(origin, destination)` â€“ Retrieves navigation routes.

* `validate(address)` â€“ Validates a given address.



## â˜ï¸ GoogleWeather

*Retrieves weather data using Google Weather API.*

* `fetch(location)` â€“ Returns weather info for a location.

* `resolve_location(query)` â€“ Performs geocoding to determine a location from a query.



## ğŸ•°ï¸ NavalObservatory

*Fetches astronomical and time data from the U.S. Naval Observatory.*

* `fetch_julian_date()` â€“ Returns current Julian date.

* `fetch_sidereal_time()` â€“ Returns local sidereal time.


## ğŸ›°ï¸ SatelliteCenter

*Interfaces with NASA SSCWeb for satellite and ground station data.*

* `fetch_orbits(satellite, start, end)` â€“ Retrieves orbital tracks for a satellite.

* `fetch_ground_stations()` â€“ Lists ground station metadata.



## ğŸŒ‹ EarthObservatory

*Connects to NASA EONET for global natural event data.*

* `fetch_events(count)` â€“ Returns recent global events (fires, storms, volcanoes, etc).

* `fetch_categories()` â€“ Returns the event categories.



## ğŸ—¾ GlobalImagery

*Pulls satellite imagery from NASA GIBS WMS.*

* `fetch_imagery(bbox, date)` â€“ Returns satellite map tiles or images.



## â˜„ï¸ NearbyObjects

*Retrieves near-Earth object (NEO) and fireball data from JPLâ€™s CNEOS/SSD APIs.*

* `fetch_neos(start, end)` â€“ Returns near-Earth object data for date range.

* `fetch_fireballs(start, end)` â€“ Returns fireball events for date range.



## ğŸ•¸ï¸ Scrapers

## ğŸ§© Extractor

*Abstract base for HTML â†’ plain-text extraction.*

* `__init__(self, raw_html: str = '')` â€” Initialize with optional raw HTML to extract.

* `extract(self)` â€” Abstract method for extracting readable text from HTML. Must be implemented by subclasses.



## ğŸ•·ï¸ WebExtractor

*Concrete, synchronous extractor using `requests` and BeautifulSoup for HTML â†’ text extraction.*

* `__init__(self, raw_html: str = '')` â€” Initialize the extractor, optionally with raw HTML.

* `fetch(self, url: str, time: int = 10)` â€” Performs an HTTP GET, returns a canonicalized Result.

* `html_to_text(self, html: str)` â€” Converts HTML to plain, readable text, removing scripts/styles.

* `scrape_paragraphs(self, uri: str)` â€” Extracts all `<p>` blocks from the page at the given URI.

* `scrape_lists(self, uri: str)` â€” Extracts all `<li>` items from lists.

* `scrape_tables(self, uri: str)` â€” Extracts and flattens all table cell contents.

* `scrape_articles(self, uri: str)` â€” Extracts text from `<article>` elements.

* `scrape_headings(self, uri: str)` â€” Extracts all headings (`<h1>`â€“`<h6>`).

* `scrape_divisions(self, uri: str)` â€” Extracts text from `<div>` elements.

* `scrape_sections(self, uri: str)` â€” Extracts text from `<section>` elements.

* `scrape_blockquotes(self, uri: str)` â€” Extracts text from `<blockquote>` elements.

* `scrape_hyperlinks(self, uri: str)` â€” Extracts all hyperlinks (from `<a href>`).

* `scrape_images(self, uri: str)` â€” Extracts image sources (from `<img src>`).

* `create_schema(self, function, tool, description, parameters, required)` â€” Builds an OpenAI Tool API schema dynamically for a function.



## ğŸ“¦ Dependencies

| Package           | Purpose/Description          | Link                                                    |
| ----------------- | ---------------------------- | ------------------------------------------------------- |
| beautifulsoup4    | HTML/XML parsing             | [PyPI](https://pypi.org/project/beautifulsoup4/)        |
| requests          | HTTP client                  | [PyPI](https://pypi.org/project/requests/)              |
| playwright        | Headless browser automation  | [PyPI](https://pypi.org/project/playwright/)            |
| langchain         | LLM & RAG framework          | [LangChain](https://python.langchain.com/)              |
| chromadb          | Vector DB for embeddings     | [PyPI](https://pypi.org/project/chromadb/)              |
| pandas            | Data analysis                | [PyPI](https://pypi.org/project/pandas/)                |
| numpy             | Numeric computing            | [PyPI](https://pypi.org/project/numpy/)                 |
| matplotlib        | Visualization                | [PyPI](https://pypi.org/project/matplotlib/)            |
| owslib            | Geospatial Web Services      | [PyPI](https://pypi.org/project/OWSLib/)                |
| astroquery        | Astronomy data               | [PyPI](https://pypi.org/project/astroquery/)            |
| unstructured      | Document parsing             | [Docs](https://unstructured-io.github.io/unstructured/) |
| pytube            | YouTube video download       | [PyPI](https://pypi.org/project/pytube/)                |
| docx2txt          | DOCX text extraction         | [PyPI](https://pypi.org/project/docx2txt/)              |
| pillow            | Image processing             | [PyPI](https://pypi.org/project/Pillow/)                |
| python-pptx       | PowerPoint processing        | [PyPI](https://pypi.org/project/python-pptx/)           |
| PyMuPDF (fitz)    | PDF parsing                  | [PyPI](https://pypi.org/project/PyMuPDF/)               |
| scikit-learn      | Machine learning             | [PyPI](https://pypi.org/project/scikit-learn/)          |
| tiktoken          | OpenAI tokenization          | [PyPI](https://pypi.org/project/tiktoken/)              |
| pyyaml            | YAML file parsing            | [PyPI](https://pypi.org/project/PyYAML/)                |
| tabulate          | Tabular text/markdown output | [PyPI](https://pypi.org/project/tabulate/)              |
| python-dotenv     | Manage .env files            | [PyPI](https://pypi.org/project/python-dotenv/)         |
| typing_extensions | Type hinting support         | [PyPI](https://pypi.org/project/typing-extensions/)     |



## âš™ï¸ Technical Notes

* Pluggable, modular pipelineâ€”add new fetchers/loaders by subclassing.

* Type-safety and error handling by design.

* Compatible with CI/CD and production data environments.



## ğŸ“ License

- MIT License [here](https://github.com/is-leeroy-jenkins/Foo/blob/main/LICENSE.txt)

- Copyright Â© 2022â€“2025 Terry D. Eppler



## ğŸ™ Acknowledgments

* Project lead: Terry D. Eppler ([terryeppler@gmail.com](mailto:terryeppler@gmail.com))

* Inspired by open-source Python, ML, and LLM communities.
